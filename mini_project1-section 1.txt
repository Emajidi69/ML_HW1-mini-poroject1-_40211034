# Mini_project1
# Mohammad_Sefid 40206864
# Erfan_Majidi 40211034

import numpy as np
import pandas as pd
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import PolynomialFeatures
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.model_selection import GridSearchCV
from sklearn.impute import SimpleImputer

#1.1
# upload data
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/weather_prediction/weather_prediction_dataset.csv')

# 2.1.1
# choose french cities
french_cities = ['TOURS', 'PERPIGNAN', 'MONTELIMAR']
columns_to_keep = ['DATE', 'MONTH']
for city in french_cities:
    columns_to_keep.extend([col for col in df.columns if city in col])
df = df[columns_to_keep]

# store filtered data
df.to_csv('french_cities_weather.csv', index=False)

# 3.1.1
num_samples = df.shape[0]
print(f"Number of samples: {num_samples}")
start_date = df['DATE'].min()
end_date = df['DATE'].max()
print(f"Data time slot: {start_date} to {end_date}")

# drop missing data
df['DATE'] = pd.to_datetime(df['DATE'], format='%Y%m%d')
df = df.dropna()

# normalize numerical data
scaler = StandardScaler()
numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
df[numeric_columns] = scaler.fit_transform(df[numeric_columns])

# extract year, month and day of date
df['YEAR'] = df['DATE'].dt.year
df['MONTH'] = df['DATE'].dt.month
df['DAY'] = df['DATE'].dt.day

# divide data to train and test
train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)

# remove outlier data
def remove_outliers(df, column):
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]

for column in numeric_columns:
    train_df = remove_outliers(train_df, column)

# store preprocessed data
train_df.to_csv('preprocessed_train_data.csv', index=False)
test_df.to_csv('preprocessed_test_data.csv', index=False)

# 4.1.1
# divide data based on Date to test & train
test_df = df[df['DATE'].dt.year == 2009]
train_df = df[df['DATE'].dt.year != 2009]

# create time windows
def create_windows(data, window_size, overlap):
    windows = []
    targets = []
    for i in range(0, len(data) - window_size, window_size - overlap):
        windows.append(data[i:i + window_size])
        targets.append(data[i + window_size])
    return np.array(windows), np.array(targets)

window_size = 5
overlap = 4

drop_columns = ['DATE', 'MONTH']
X_train, y_train = create_windows(train_df.drop(columns=drop_columns).values, window_size, overlap)
X_test, y_test = create_windows(test_df.drop(columns=drop_columns).values, window_size, overlap)

# model with using time(t-1)
X_train_t1 = X_train[:, -1, :]
X_test_t1 = X_test[:, -1, :]
model_t1 = LinearRegression()
model_t1.fit(X_train_t1, y_train)
score_t1 = model_t1.score(X_test_t1, y_test)

# model with using time slot(t-n to t-1)
X_train_tn = X_train.reshape(X_train.shape[0], -1)
X_test_tn = X_test.reshape(X_test.shape[0], -1)
model_tn = LinearRegression()
model_tn.fit(X_train_tn, y_train)
score_tn = model_tn.score(X_test_tn, y_test)

print(f"Comparing the performance of the models:")
print(f"performance mode (t-1): {score_t1}")
print(f"performance mode (t-n to t-1): {score_tn}")

# 2.1
# choose columns related to TOURS city
Tours_columns = [col for col in df.columns if 'TOURS' in col]
Tours_columns.insert(0, 'DATE')
Tours_df = df[Tours_columns]

# divide data of Tours to train and test
train_Tours_df, test_Tours_df = train_test_split(Tours_df, test_size=0.2, random_state=42)

# 3.1
# define function to create polyomial features
# Drop only the columns that exist in the DataFrame
columns_to_drop = ['DATE', 'TOURS_temp_mean']  # Removed 'MONTH' as it's not present
if 'MONTH' in train_Tours_df.columns:
    columns_to_drop.append('MONTH')

X_train = train_Tours_df.drop(columns=columns_to_drop).values
X_test = test_Tours_df.drop(columns=columns_to_drop).values  # Apply the same to X_test
y_train = train_Tours_df['TOURS_temp_mean'].values
y_test = test_Tours_df['TOURS_temp_mean'].values

def create_polynomial_features(X, degree):
    n_samples, n_features = X.shape
    features = [np.ones(n_samples)]  # Add bias term (1s for the intercept term)

    for d in range(1, degree + 1):
        for j in range(n_features):
            features.append(X[:, j] ** d)

    return np.column_stack(features)

# define function to predict weights
def predict(X, weights):
    return X @ weights

# define function to calculate gradient
def compute_gradient(X, y_true, y_pred):
    error = y_pred - y_true
    gradient = X.T @ error / len(y_true)
    return gradient

def train_polynomial_regression(X_train, y_train, X_test, y_test, degree, learning_rate=0.01, max_epochs=1000, tolerance=1e-4):
    X_train_poly = create_polynomial_features(X_train, degree)
    X_test_poly = create_polynomial_features(X_test, degree)

    # Normalize data
    scaler = StandardScaler()
    X_train_poly = scaler.fit_transform(X_train_poly)
    X_test_poly = scaler.transform(X_test_poly)

    # Check for NaN and inf values in X_train_poly and y_train
    if np.any(np.isnan(X_train_poly)) or np.any(np.isnan(y_train)):
        print("NaN values found in X_train_poly or y_train!")
    if np.any(np.isinf(X_train_poly)) or np.any(np.isinf(y_train)):
        print("Infinite values found in X_train_poly or y_train!")

    # Initialize weights randomly to avoid starting at zero
    weights = np.random.randn(X_train_poly.shape[1]) * 0.01

    # Store errors
    train_errors = []
    test_errors = []

    # Train model loop
    progress_bar = tqdm(range(max_epochs), desc="Training Progress")
    for epoch in progress_bar:
        # Output prediction
        y_pred_train = predict(X_train_poly, weights)
        y_pred_test = predict(X_test_poly, weights)

        # Calculate error
        train_error = mean_absolute_percentage_error(y_train, y_pred_train)
        test_error = mean_absolute_percentage_error(y_test, y_pred_test)

        # Store error
        train_errors.append(train_error)
        test_errors.append(test_error)

        # Show progress bar for error
        progress_bar.set_postfix(train_error=train_error, test_error=test_error)

        # Check stop train condition
        if train_error < tolerance:
            print(f"Training stopped at epoch {epoch} with train error: {train_error}")
            break

        # Calculate gradient & update weights
        error = y_pred_train - y_train
        if np.any(np.isnan(error)) or np.any(np.isinf(error)):
            print("NaN or inf in error!")
            break

        gradient = np.dot(X_train_poly.T, error) / len(y_train)
        weights -= learning_rate * gradient

    return weights, train_errors, test_errors

# initialize variales
max_epochs = 1000
tolerance = 1e-4
best_error = float('inf')
error_improvement = tolerance
degree = 2
learning_rate = 0.01

# Initialize the model before the training loop
model = LinearRegression()

# Fit the model initially before the loop starts
model.fit(X_train, y_train)

# progress tape
pbar = tqdm(range(max_epochs), desc="Training Progress")

# training loop
for epoch in pbar:
    # calculate prediction & erroe
    y_pred = model.predict(X_train)
    train_error = np.mean((y_train - y_pred)**2)

    # calculate Test Error
    y_test_pred = model.predict(X_test)
    test_error = np.mean((y_test - y_test_pred)**2)

    # update the progress bar
    pbar.set_postfix(Train_Error=train_error, Test_Error=test_error)

    # check error enhancement for stop training
    if abs(best_error - test_error) < error_improvement:
        print(f"Stopping training early at epoch {epoch+1} as error improvement is below tolerance.")
        break

    best_error = test_error

# train model
weights, train_errors, test_errors = train_polynomial_regression(X_train, y_train, X_test,
                                     y_test, degree, learning_rate, max_epochs, tolerance)

# show errors
plt.plot(train_errors, label="Train Error")
plt.plot(test_errors, label="Test Error")
plt.xlabel("Epoch")
plt.ylabel("MAPE(%)")
plt.title("Training and Test Error Over Epochs")
plt.legend()
plt.show()

# define funtion to calculate MAPE
y_pred = model.predict(X_test)
mape = mean_absolute_percentage_error(y_test, y_pred)
print(f"optimized model MAPE : {mape}")

# 1.3.1
# =================================
# Simulate train/test
# =================================
X = df[Tours_columns]
y = df['TOURS_temp_mean']  # Using TOURS_temp_mean as an example target

# divide data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)

# ========================
# Standardize Input Data
# ========================
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.drop(columns=['DATE']))
X_test_scaled = scaler.transform(X_test.drop(columns=['DATE']))

# ========================
# Linear Regression
# ========================
mlr = LinearRegression()
mlr.fit(X_train_scaled, y_train)
y_pred_mlr = mlr.predict(X_test_scaled)
mape_mlr = mean_absolute_percentage_error(y_test, y_pred_mlr) * 100

# ========================
# Ridge Regression (with GridSearchCV)
# ========================
ridge_params = {'alpha': np.logspace(-3, 3, 20)}
ridge = Ridge()
ridge_cv = GridSearchCV(ridge, ridge_params, cv=5)
ridge_cv.fit(X_train_scaled, y_train)
best_ridge = ridge_cv.best_estimator_
y_pred_ridge = best_ridge.predict(X_test_scaled)
mape_ridge = mean_absolute_percentage_error(y_test, y_pred_ridge) * 100

# ========================
# Lasso Regression (with GridSearchCV)
# ========================
lasso_params = {'alpha': np.logspace(-3, 3, 20)}
lasso = Lasso(max_iter=5000)
lasso_cv = GridSearchCV(lasso, lasso_params, cv=5)
lasso_cv.fit(X_train_scaled, y_train)
best_lasso = lasso_cv.best_estimator_
y_pred_lasso = best_lasso.predict(X_test_scaled)
mape_lasso = mean_absolute_percentage_error(y_test, y_pred_lasso) * 100

# define models
models = {
    'Linear Regression': LinearRegression(),
    'Ridge Regression': Ridge(alpha=1.0),
    'Lasso Regression': Lasso(alpha=0.1)
}

results = {}
print(f"\n--- TOURS ---")

for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_pred = model.predict(X_test_scaled)
    mape = mean_absolute_percentage_error(y_test, y_pred) * 100
    results[name] = mape
    print(f"{name} - MAPE: {mape:.2f}%")

# show best model
best_model = min(results, key=results.get)
print(f"\n Best Model: {best_model} with MAPE = {results[best_model]:.2f}%")

# بخش امتیازی
# ========== STEP 1: Train on two individual cities ==========
montelimar_features = [col for col in df.columns if 'MONTELIMAR' in col]
X = df[montelimar_features]
y = df['MONTELIMAR_temp_mean']

X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=['MONTELIMAR_temp_mean']), y, test_size=0.2, shuffle=False)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression
mlr = LinearRegression()
mlr.fit(X_train_scaled, y_train)
y_pred_mlr = mlr.predict(X_test_scaled)
mape_mlr = mean_absolute_percentage_error(y_test, y_pred_mlr) * 100

# Ridge
ridge_params = {'alpha': np.logspace(-3, 3, 20)}
ridge_cv = GridSearchCV(Ridge(), ridge_params, cv=5)
ridge_cv.fit(X_train_scaled, y_train)
best_ridge = ridge_cv.best_estimator_
mape_ridge = mean_absolute_percentage_error(y_test, best_ridge.predict(X_test_scaled)) * 100

# Lasso
lasso_params = {'alpha': np.logspace(-3, 3, 20)}
lasso_cv = GridSearchCV(Lasso(max_iter=5000), lasso_params, cv=5)
lasso_cv.fit(X_train_scaled, y_train)
best_lasso = lasso_cv.best_estimator_
mape_lasso = mean_absolute_percentage_error(y_test, best_lasso.predict(X_test_scaled)) * 100

print(f"\n--- MONTELIMAR ---")
print(f"Linear Regression MAPE: {mape_mlr:.2f}%")
print(f"Ridge Regression MAPE: {mape_ridge:.2f}%")
print(f"Lasso Regression MAPE: {mape_lasso:.2f}%")

perpignan_features = [col for col in df.columns if 'PERPIGNAN' in col]
X = df[perpignan_features]
y = df['PERPIGNAN_temp_mean']

X_train, X_test, y_train, y_test = train_test_split(X.drop(columns=['PERPIGNAN_temp_mean']), y, test_size=0.2, shuffle=False)

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Linear Regression
mlr = LinearRegression()
mlr.fit(X_train_scaled, y_train)
y_pred_mlr = mlr.predict(X_test_scaled)
mape_mlr = mean_absolute_percentage_error(y_test, y_pred_mlr) * 100

# Ridge
ridge_params = {'alpha': np.logspace(-3, 3, 20)}
ridge_cv = GridSearchCV(Ridge(), ridge_params, cv=5)
ridge_cv.fit(X_train_scaled, y_train)
best_ridge = ridge_cv.best_estimator_
mape_ridge = mean_absolute_percentage_error(y_test, best_ridge.predict(X_test_scaled)) * 100

# Lasso
lasso_params = {'alpha': np.logspace(-3, 3, 20)}
lasso_cv = GridSearchCV(Lasso(max_iter=5000), lasso_params, cv=5)
lasso_cv.fit(X_train_scaled, y_train)
best_lasso = lasso_cv.best_estimator_
mape_lasso = mean_absolute_percentage_error(y_test, best_lasso.predict(X_test_scaled)) * 100

print(f"\n--- PERPIGNAN ---")
print(f"Linear Regression MAPE: {mape_mlr:.2f}%")
print(f"Ridge Regression MAPE: {mape_ridge:.2f}%")
print(f"Lasso Regression MAPE: {mape_lasso:.2f}%")

# ========== STEP 2: Train on 3 cities and test on 1 ==========
# بازخوانی داده‌ها
df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/weather_prediction/weather_prediction_dataset.csv')

# تنظیمات اولیه
cities = ['BASEL', 'DE_BILT', 'DUSSELDORF']
features = ['cloud_cover', 'humidity', 'pressure', 'global_radiation', 'precipitation', 'sunshine', 'temp_min', 'temp_max']
target = 'temp_mean'

# Define all_columns correctly here
all_columns = ['DATE']  # Start with 'DATE'
for city in cities:
    all_columns.extend([f"{city}_{feature}" for feature in features])
    all_columns.append(f"{city}_{target}")


X_data = []
y_data = []

for city in cities:
    city_features = [f"{city}_{feature}" for feature in features]
    city_target = f"{city}_{target}"

    # استخراج داده‌ها و حذف ردیف‌های ناقص
    city_df = df[all_columns].dropna() # Now all_columns is correctly defined

    # اضافه کردن به مجموعه کلی
    X_data.append(city_df[city_features])
    y_data.append(city_df[city_target])

# ترکیب داده‌ها
X = pd.concat(X_data, ignore_index=True)
y = pd.concat(y_data, ignore_index=True)

# حذف مقادیر بسیار کوچک یا صفر در y برای MAPE
mask = y.abs() > 1e-6
X = X[mask]
y = y[mask]

# تقسیم‌بندی داده‌ها
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Impute NaN values before scaling
from sklearn.impute import SimpleImputer
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)
X_test = imputer.transform(X_test)

# استانداردسازی
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# تابع MAPE امن
import numpy as np
def safe_mape(y_true, y_pred):
    y_true, y_pred = np.array(y_true), np.array(y_pred)
    mask = np.abs(y_true) > 1e-6
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / (y_true[mask] + 1e-6))) * 100

# ارزیابی مدل‌ها
results = {}
for name, model in models.items():
    model.fit(X_train_scaled, y_train)
    y_train_pred = model.predict(X_train_scaled)
    y_test_pred = model.predict(X_test_scaled)
    results[name] = {
        'Train MAPE (%)': safe_mape(y_train, y_train_pred),
        'Test MAPE (%)': safe_mape(y_test, y_test_pred)
    }

# نمایش نتایج
for model_name, metrics in results.items():
    print(f"\n✅ {model_name}")
    print(f"Train MAPE: {metrics['Train MAPE (%)']:.2f}%")
    print(f"Test MAPE: {metrics['Test MAPE (%)']:.2f}%")
